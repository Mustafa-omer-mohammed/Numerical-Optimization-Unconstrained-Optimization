We tested the three algorithms on three problems in N-D space, where N = [103,104,105]. For each problem we calculated
analytically the gradient and the hessian in order to apply the chosen methods. We optimize successfully each function
with at least one method.
We analyzed the performances of each method for all the problems, in terms of the speed of convergence and the computational cost of each iteration, the accuracy 
of the solution compared to the actual minimum of the function. We decide to compute the gradient and the hessian analytically, because using finite differences would 
increase the computational cost of each single iteration.
